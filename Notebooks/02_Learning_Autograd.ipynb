{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "IUZhml_gmwIL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in deep neural networks we have to find a lot of\n",
        "#derivatives to update weights and biases and it becomes\n",
        "#very difficult to manually calculate\n",
        "\n",
        "#Autograd helps us to automically perform differentiation"
      ],
      "metadata": {
        "id": "SnfseSCSnPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "GjBGZES3qNrK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML6sKqTpqVEw",
        "outputId": "98de0968-95ac-4374-da77-8b6b6c2933cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "oUfNIVa4qk58",
        "outputId": "c7c8fd80-005f-4a89-dea5-ceb556a517c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Tensor.backward of tensor(9., grad_fn=<PowBackward0>)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch._tensor.Tensor.backward</b><br/>def backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torch/_tensor.py</a>Computes the gradient of current tensor wrt graph leaves.\n",
              "\n",
              "The graph is differentiated using the chain rule. If the tensor is\n",
              "non-scalar (i.e. its data has more than one element) and requires\n",
              "gradient, the function additionally requires specifying a ``gradient``.\n",
              "It should be a tensor of matching type and shape, that represents\n",
              "the gradient of the differentiated function w.r.t. ``self``.\n",
              "\n",
              "This function accumulates gradients in the leaves - you might need to zero\n",
              "``.grad`` attributes or set them to ``None`` before calling it.\n",
              "See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`\n",
              "for details on the memory layout of accumulated gradients.\n",
              "\n",
              ".. note::\n",
              "\n",
              "    If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
              "    in a user-specified CUDA stream context, see\n",
              "    :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.\n",
              "\n",
              ".. note::\n",
              "\n",
              "    When ``inputs`` are provided and a given input is not a leaf,\n",
              "    the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
              "    It is an implementation detail on which the user should not rely.\n",
              "    See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
              "\n",
              "Args:\n",
              "    gradient (Tensor, optional): The gradient of the function\n",
              "        being differentiated w.r.t. ``self``.\n",
              "        This argument can be omitted if ``self`` is a scalar.\n",
              "    retain_graph (bool, optional): If ``False``, the graph used to compute\n",
              "        the grads will be freed. Note that in nearly all cases setting\n",
              "        this option to True is not needed and often can be worked around\n",
              "        in a much more efficient way. Defaults to the value of\n",
              "        ``create_graph``.\n",
              "    create_graph (bool, optional): If ``True``, graph of the derivative will\n",
              "        be constructed, allowing to compute higher order derivative\n",
              "        products. Defaults to ``False``.\n",
              "    inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n",
              "        accumulated into ``.grad``. All other tensors will be ignored. If not\n",
              "        provided, the gradient is accumulated into all the leaf Tensors that were\n",
              "        used to compute the :attr:`tensors`.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 525);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWa94nKqq8B9",
        "outputId": "0edf3bb1-dc58-4582-b961-ee67c6c9fbac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(4.0,requires_grad=True)\n",
        "y = x**2\n",
        "z = torch.sin(y)\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T21ZI6djrGtV",
        "outputId": "f3fde571-3e7b-481f-f94d-868db05f1a9f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., requires_grad=True)\n",
            "tensor(16., grad_fn=<PowBackward0>)\n",
            "tensor(-0.2879, grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "cZl7_eoRr0EE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4JGwAt1r20E",
        "outputId": "412022f6-330a-4786-82d4-53ccaa3e00c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-7.6613)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "wfFVt0C6r4Oc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0,requires_grad=True)\n",
        "b = torch.tensor(0.0,requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjrZ615fuBJ-",
        "outputId": "dc202a0f-c462-470f-f4ff-cf37a6e153d6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., requires_grad=True)\n",
            "tensor(0., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy_loss(prediction,target):\n",
        "  epsilon = 1e-8\n",
        "  prediction = torch.clamp(prediction,epsilon,1-epsilon)\n",
        "  return -(target*torch.log(prediction) + (1-target)*torch.log(1-prediction))"
      ],
      "metadata": {
        "id": "fXvwFcmZu2V-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = w*x + b\n",
        "y_pred = torch.sigmoid(z)\n",
        "loss = binary_cross_entropy_loss(y_pred,y)"
      ],
      "metadata": {
        "id": "rO22jBrFuLxf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "GoQTNtjgukMq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Omrsu4dutvC",
        "outputId": "797ffbde-c012-4746-cd9a-63968e977c63"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
        "y = (x**2).mean()"
      ],
      "metadata": {
        "id": "Dw9-K70DviEh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "00dicUqNwLc_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad # we get dy/dx1 , dy/dx2 , dy/dx3 with partial differentiation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn6F0rBuwN8f",
        "outputId": "6be5efe8-25be-4cc8-84ee-70cad2ed0269"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clearing gradients\n",
        "#after rerunning forward and backward the gradients accumulate rather than clearing\n",
        "#This would cause error so we clear gradient before another pass\n",
        "\n",
        "#x.grad.zero_()\n",
        "\n",
        "#disabling gradient tracking\n",
        "#useful when model is predicting and we dont need to find derivative\n",
        "\n",
        "#option 1 = requires_grad(False)\n",
        "#option 2 = detach()\n",
        "#option 3 = torch.no_grad()"
      ],
      "metadata": {
        "id": "JTEGRYBrwO7_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00l2cLxEx3O8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}